{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/sentiment_comp_qaie_pairs.pkl','rb') as f:\n",
    "    pairs = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12325\n",
      "12272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['智通财经APP讯，太平洋网络(00543)发布公告，将于2023年6月12日派发截至2022年12月31日止年度的末期股息每股0.1元人民币。\\n---\\n请从上文中抽取出所有公司/机构、对应的在本文中的情感倾向（积极、消极、中性）以及原因。\\n并用这样的格式返回：\\n{\"ORG\":..., \"sentiment\":..., \"reason\":...}',\n",
       " '{\"ORG\": \"太平洋网络\", \"sentiment\": \"积极\", \"reason\": \"宣布派发股息每股0.1元人民币\"}']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "pairs = [p for p in pairs if p[1] not in ['无','']]\n",
    "print(len(pairs))\n",
    "\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/sentiment_comp_ie.json','w',encoding='utf8') as f:\n",
    "    for p in pairs:\n",
    "        line = {\"q\":p[0],\"a\":p[1]}\n",
    "        f.write(json.dumps(line,ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱顺序重新排列\n",
    "import json\n",
    "import random\n",
    "with open('data/sentiment_comp_ie_shuffled.json','w',encoding='utf8') as f:\n",
    "    random_pairs = random.sample(pairs,k=len(pairs))\n",
    "    for p in random_pairs:\n",
    "        line = {\"q\":p[0],\"a\":p[1]}\n",
    "        f.write(json.dumps(line,ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baichuan-7B inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 下载百川大模型看看\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试从大模型中获取情感标签的概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"问：今天心情还真不错。这句话的情感是怎样的？答：这句话的情感是：\"\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to('cuda:0')\n",
    "# outputs = model.generate(inputs,max_new_tokens=50,do_sample=True)\n",
    "# output = outputs[0][len(inputs[0]):]\n",
    "# tokenizer.decode(output,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['消极', '负面', '难过'] score(prob*100) = 0.2\n",
      "1 ['积极', '正面', '开心'] score(prob*100) = 0.07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"ChatGPT的推出对百度的搜索业务产生的强烈冲击，搜索引擎的作用性在降低\"\n",
    "\n",
    "prompt = f\"问：{text}这句话的情感是怎样的？答：这句话的情感是：\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt').to('cuda:0')\n",
    "\n",
    "last_logits = model(inputs).logits[:,-1,:]\n",
    "probs = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "targets = {\n",
    "    0: ['消极','负面','难过'],\n",
    "    1: ['积极','正面','开心']\n",
    "}\n",
    "for key in targets:\n",
    "    target_probs = []\n",
    "    for w in targets[key]:\n",
    "        # 取出非 bos 的第一个token（似乎无法通过 add_special_tokens=False 去掉）\n",
    "        idx = tokenizer.encode(w)[1]\n",
    "        current_prob = probs[0, idx].item()\n",
    "        # print(w, current_prob)\n",
    "        target_probs.append(current_prob)\n",
    "    print(key,targets[key],'score(prob*100) =',round(sum(target_probs)*100/len(target_probs),2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不耐烦，极其不耐烦。为什么会不耐烦呢？因为我们总是觉得对方不了解情况。1 先了解一下事情的始末吧！在说这句话的时候，对方可能会说：“事情不是这样子的呀'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs,max_new_tokens=50,do_sample=True)\n",
    "output = outputs[0][len(inputs[0]):]\n",
    "tokenizer.decode(output,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"weights/sentiment_comp_ie_shuffled_baichuan-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ChatGPT的提出对谷嘎、万度的搜索业务产生巨大打击，传统搜索引擎的作用性降低了。\n",
    "与此同时，OChat，Linguo等新兴语义搜索公司，迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐。\n",
    "腾势、艾里等公司表示会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型。\n",
    "大型图片供应商视觉中国称ChatGPT对公司业务暂无影响，还在观望状态。\n",
    "（本文图片来自视觉中国，上观新闻为您报道。）\n",
    "更多报道：\n",
    "- 亚牛逊公司关于AIGC的表态\n",
    "- 巨硬公司昨日在A股上市\n",
    "---\n",
    "请从上文中抽取出所有公司，以及对应的在本文中的情感倾向（积极、消极、中性）以及原因。\n",
    "请用这样的格式返回：\n",
    "{\"ORG\":..., \"sentiment\":..., \"reason\":...}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. {\"ORG\":\"百度\",\"SENTIMENT\":\"+20%\", \"REASON\":\"新产品发布/更新;技术突破;行业领先地位提升;市场份额扩大;营收增长;股价上涨\"; }\n",
      "2. {\"ORG\":\"阿里巴巴集团\",\"SENTIMENT\":\"-50%\", \"REASON\":\"竞争对手发展迅猛;业绩下滑;市值缩水;股价下跌\"; }\n",
      "3. {\"ORG\":\"腾讯控股有限公司\",\"SENTIMENT\":\"+40%\", \"REASON\":\"新产品发布\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = inputs.to('cuda:0')\n",
    "output = model.generate(**inputs, max_new_tokens=128,repetition_penalty=1.1, streamer=streamer)\n",
    "# 模型非常自信！（类似于模型自动纠错的能力）同时幻觉十分严重，哈哈哈（当然，这只是训练了200steps）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "device=torch.device(2)\n",
    "\n",
    "model_path = \"THUDM/chatglm-6b\"\n",
    "# model_glm = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)\n",
    "model_glm = AutoModel.from_pretrained(model_path, trust_remote_code=True,device_map='auto')\n",
    "tokenizer_glm = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LoRA\n",
    "model_glm = PeftModel.from_pretrained(model_glm, \"weights/sentiment_comp_ie\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不确定您所指的“艾里巴巴公司”是指哪个公司。如果您能提供更多上下文或信息,我将尽力回答您的问题。\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer_glm,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer_glm(\"我们说的艾里巴巴公司，指的是\", return_tensors='pt')\n",
    "inputs = inputs.to('cuda:2')\n",
    "output = model_glm.generate(**inputs, max_new_tokens=1024,repetition_penalty=1.1, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "融资资金将用于完善生成式AI新引擎的构建。\n",
      "\n",
      "创业邦获悉，自然语言技术AI服务商竹间智能宣布已完成D2轮融资，由金浦投资、金库资本、江苏文投、隽赐资本等联合投资。至今，竹间智能已累计完成7轮融资，陆续引入科沃斯、云晖资本、中银国际等股东机构。值得一提的是，竹间智能已经正式推出运用类ChatGPT技术的成熟AIGC产品，并即将开启新一轮融资。\n",
      "\n",
      "融资资金将用于完善生成式AI新引擎的构建，把以ChatGPT为代表的大语言模型和AIGC技术全面融入竹间产品体系，结合大小模型构建双引擎驱动产品迭代和技术升级，平衡大小模型的优缺点，全面焕新产品功能和服务模式，并正式将产品同步推向海外市场，成为服务全球企业和用户的跨国NLP能力厂商。\n",
      "\n",
      "竹间智能由前微软（亚洲）互联网工程院副院长简仁贤于2015年创办，致力于以自然语言处理、情感计算、深度学习、知识工程、文本处理等人工智能技术为基础，将AI能力惠及千行百业。\n",
      "\n",
      "---\n",
      "请从上文中抽取出所有公司，以及对应的在本文中的情感倾向（积极、消极、中性）以及原因。\n",
      "请用这样的格式返回：\n",
      "{\"ORG\":..., \"sentiment\":..., \"reason\":...}\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer自动把中文括号转成英文括号了...所以你直接在输出里面可能无法直接匹配到 prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda:0')\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device_map='auto')\n",
    "# model = model.eval()\n",
    "# model\n",
    "# response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "# print(response)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hf_device_map\n",
    "model.hf_device_map['transformer.output_layer'] = model.hf_device_map['transformer.embedding']\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device_map=model.hf_device_map)\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer('你好',return_tensors='pt')\n",
    "res_glm = tokenizer_glm('你好',return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = res['input_ids'].clone()\n",
    "model(input_ids=res['input_ids'],labels=labels)\n",
    "\n",
    "# model_glm = model_glm.half()\n",
    "# labels_glm = res_glm['input_ids'].clone()\n",
    "# model_glm(input_ids=res_glm['input_ids'],labels=labels_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对比一下 ChatGLM 跟 ChatGLM2 的结构差别：**\n",
    "\n",
    "ChatGLM:\n",
    "```python\n",
    "ChatGLMForConditionalGeneration(\n",
    "  (transformer): ChatGLMModel(\n",
    "    (word_embeddings): Embedding(130528, 4096)\n",
    "    (layers): ModuleList(\n",
    "      (0-27): 28 x GLMBlock(\n",
    "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "        (attention): SelfAttention(\n",
    "          (rotary_emb): RotaryEmbedding()\n",
    "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
    "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        )\n",
    "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GLU(\n",
    "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
    "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
    "```\n",
    "\n",
    "ChatGLM2:\n",
    "```python\n",
    "ChatGLMForConditionalGeneration(\n",
    "  (transformer): ChatGLMModel(\n",
    "    (embedding): Embedding(\n",
    "      (word_embeddings): Embedding(65024, 4096)   # <-- smaller vocab size\n",
    "    )\n",
    "    (rotary_pos_emb): RotaryEmbedding()\n",
    "    (encoder): GLMTransformer(\n",
    "      (layers): ModuleList(\n",
    "        (0-27): 28 x GLMBlock(\n",
    "          (input_layernorm): RMSNorm()   # <-- LayerNorm to RMSNorm\n",
    "          (self_attention): SelfAttention(\n",
    "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)   # <-- smaller attention out_features\n",
    "            (core_attention): CoreAttention(\n",
    "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          )\n",
    "          (post_attention_layernorm): RMSNorm()\n",
    "          (mlp): MLP(                # <-- GLU to MLP\n",
    "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
    "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (final_layernorm): RMSNorm()\n",
    "    )\n",
    "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)    # <-- smaller out_features\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=130528, bias=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glm.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=65024, bias=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ORG\": \"ChatGPT\", \"sentiment\": \"消极\", \"reason\": \"ChatGPT的提出对谷嘎、万度的搜索业务产生巨大打击，传统搜索引擎的作用性降低了。\"},\n",
      "{\"ORG\": \"OChat\", \"sentiment\": \"积极\", \"reason\": \"OChat，Linguo等新兴语义搜索公司，迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐。\"},\n",
      "{\"ORG\": \"腾势\", \"sentiment\": \"积极\", \"reason\": \"腾势、艾里等公司表示会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型。\"},\n",
      "{\"ORG\": \"视觉中国\", \"sentiment\": \"中性\", \"reason\": \"大型图片供应商视觉中国称ChatGPT对公司业务暂无影响，还在观望状态。\"},\n",
      "{\"ORG\": \"亚牛逊公司\", \"sentiment\": \"中性\", \"reason\": \"亚牛逊公司关于AIGC的表态中并未提及ChatGPT对公司业务的影响。\"},\n",
      "{\"ORG\": \"巨硬公司\", \"sentiment\": \"中性\", \"reason\": \"巨硬公司昨日在A股上市与ChatGPT对公司业务的影响无关。\"}\n"
     ]
    }
   ],
   "source": [
    "print(model.chat(tokenizer,query=text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"weights/sentiment_comp_ie_chatglm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ORG\": \"谷嘎\", \"sentiment\": \"消极\", \"reason\": \"ChatGPT的提出对其搜索业务产生巨大打击\"}\n",
      "{\"ORG\": \"万度\", \"sentiment\": \"消极\", \"reason\": \"传统搜索引擎的作用性降低了\"}\n",
      "{\"ORG\": \"OChat\", \"sentiment\": \"积极\", \"reason\": \"迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐\"}\n",
      "{\"ORG\": \"Linguo\", \"sentiment\": \"积极\", \"reason\": \"迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐\"}\n",
      "{\"ORG\": \"腾势\", \"sentiment\": \"积极\", \"reason\": \"会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型\"}\n",
      "{\"ORG\": \"艾里\", \"sentiment\": \"积极\", \"reason\": \"会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型\"}\n",
      "{\"ORG\": \"视觉中国\", \"sentiment\": \"中性\", \"reason\": \"称ChatGPT对公司业务暂无影响，还在观望状态\"}\n"
     ]
    }
   ],
   "source": [
    "print(model.chat(tokenizer,text)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HC3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory /ssd3/xiaoyichao/LLM-Tuning/projects/ChatBaichuan-HC3/HC3-Chinese is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> 4\u001b[0m hc3 \u001b[39m=\u001b[39m load_from_disk(\u001b[39m'\u001b[39;49m\u001b[39m/ssd3/xiaoyichao/LLM-Tuning/projects/ChatBaichuan-HC3/HC3-Chinese\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m hc3\n",
      "File \u001b[0;32m~/.conda/envs/xyc/lib/python3.9/site-packages/datasets/load.py:2241\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[39mreturn\u001b[39;00m DatasetDict\u001b[39m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory, storage_options\u001b[39m=\u001b[39mstorage_options)\n\u001b[1;32m   2240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2242\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m{\u001b[39;00mdataset_path\u001b[39m}\u001b[39;00m\u001b[39m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2243\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory /ssd3/xiaoyichao/LLM-Tuning/projects/ChatBaichuan-HC3/HC3-Chinese is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "hc3 = load_from_disk('projects/ChatBaichuan-HC3')\n",
    "hc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '盗贼天赋盗贼怎么加天赋?知道告诉一下下啦~~ ',\n",
       " 'answer': '如果你在玩角色扮演游戏（RPG），那么你可能是在问如何在游戏中给你的盗贼角色加天赋。具体方法可能因游戏而异，但通常有以下几种方法： \\n1. 在游戏开始时选择盗贼天赋：在游戏开始时，你可以选择你想要的天赋。这通常是通过选择不同的角色种族或职业来实现的。 \\n2. 在游戏进程中获得天赋：在游戏进程中，你可能会获得一些与盗贼相关的天赋。这可能是通过完成任务、升级或解锁新的技能来实现的。 \\n3. 使用道具或装备获得天赋：你可能会发现一些道具或装备，它们可以赋予你一些盗贼天赋。这些道具或装备通常是随机生成的，或者是你在游戏进程中获得的奖励。 \\n希望这些信息对你有所帮助！'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/hc3_chatgpt_zh_specific_qa.json','w',encoding='utf8') as f:\n",
    "    for i in range(len(hc3)):\n",
    "        # line = {'q':hc3[i]['question'], 'a':hc3[i]['answer']}\n",
    "        line = {'q':'问：'+hc3[i]['question'], 'a':'答：'+hc3[i]['answer']}\n",
    "        line = json.dumps(line, ensure_ascii=False)\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True)\n",
    "import torch\n",
    "device = torch.device('cuda:2')\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True).to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "    inputs = tokenizer(\"问：\"+text+\"答：\", return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:2')\n",
    "    output = model.generate(**inputs, max_new_tokens=128,\n",
    "                            do_sample=True,\n",
    "                            repetition_penalty=1.1, \n",
    "                            begin_suppress_tokens=[tokenizer.eos_token_id],\n",
    "                            # streamer=streamer\n",
    "                            )\n",
    "    output = output[0][inputs.input_ids.shape[1]:] # 这样可以防止输出prompt部分\n",
    "    return tokenizer.decode(output,skip_special_tokens=True)\n",
    "    # return output\n",
    "    \n",
    "# chat('你怎么把断了的螺丝钉钻出来？')\n",
    "# tokenizer.eos_token_id\n",
    "# tokenizer.encode('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, \"weights/rulai_plus_baichuan-7B/\")\n",
    "model = PeftModel.from_pretrained(model, \"RLHF/weights/baichaun_rlhf_beyond_chinese_test_6step_40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有事情想找老公商量又不好意思？\n"
     ]
    }
   ],
   "source": [
    "output = chat('哎，今天好烦啊')\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len = 2500  ~ 9G\n",
    "len = 5000  ~ 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('internlm/internlm-7b', trust_remote_code=True, device_map=\"auto\") \n",
    "tokenizer = AutoTokenizer.from_pretrained('internlm/internlm-7b', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLMForCausalLM(\n",
       "  (model): InternLMModel(\n",
       "    (embed_tokens): Embedding(103168, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x InternLMDecoderLayer(\n",
       "        (self_attn): InternLMAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): InternLMRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): InternLMMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): InternLMRMSNorm()\n",
       "        (post_attention_layernorm): InternLMRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLMRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=103168, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照惯例，先观察一下模型结构\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"正向\":1,\"负向\":0,\"未提及\":0,\"中\":0,\"推荐程度\":3,\"性价比\":3,\"折扣力度\":0,\"装修情况\":3,\"嘈杂情况\":0,\"就餐空间\":3,\"卫生情况\":3,\"分量\":3,\"口感\":3,\"外观\":3,\"推荐程度\":3,\"再次消费的意愿\":3}<eoa>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import TextStreamer\n",
    "def predict(text,instruction):\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "    prompt = instruction + text + '\\n输出：'\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    output = model.generate(**inputs, max_new_tokens=1024,\n",
    "                            # do_sample=True,\n",
    "                            # repetition_penalty=1.1, \n",
    "                            temperature=0.8,\n",
    "                            top_p=0.8,\n",
    "                            eos_token_id=(2, 103028),\n",
    "                            # begin_suppress_tokens=[tokenizer.eos_token_id],\n",
    "                            streamer=streamer\n",
    "                            )\n",
    "    output = output[0][inputs.input_ids.shape[1]:] # 这样可以防止输出prompt部分\n",
    "    return tokenizer.decode(output,skip_special_tokens=True)\n",
    "\n",
    "text = \"首先是环境很舒服，我去过附近不少咖啡馆，像回声，咖啡王子一号店，星巴克等等，感觉他们家的环境绝对是顶尖的之一。\\\n",
    "星巴克适合商务人士喝杯咖啡小憩一会，回声适合安安静静的喝咖啡读书度过一个下午。这里怎么说呢，介于两者之间吧，有咖啡有西餐，\\\n",
    "    在这晃一天都没问题。对于我这单纯学习狗来说，音乐声音有点大，另外每个桌子都有电源，但是mac电脑的电源很不好插。\\\n",
    "        点了一杯香草拿铁，觉得有点甜，不过有免费的柠檬水，可以解渴。图中是芝士玉米圆饼，芝士用料很足，个人感觉不错。\\\n",
    "            有时间的话，在这里喝喝咖啡，读读书度过一个下午真是太舒服了。\"\n",
    "\n",
    "instruction = \"给定一段文字，请你进行细粒度方面情感分析，具体包括以下这些方面：交通是否便利,距离商圈远近,是否容易寻找,排队等候时间,服务人员态度,是否容易停车,点菜/上菜速度,价格水平,性价比,折扣力度,装修情况,嘈杂情况,就餐空间,卫生情况,分量,口感,外观,推荐程度,本次消费感受,再次消费的意愿。请抽取出所有方面的情感倾向：正向:1, 中性:0, 负向:-1, 未提及:-2，用json格式返回结果。 评论如下：\"\n",
    "output = predict(text, instruction)\n",
    "\n",
    "# json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"交通是否便利\": \"未提及\", \"距离商圈远近\": \"未提及\", \"是否容易寻找\": \"未提及\", \"排队等候时间\": \"未提及\", \"服务人员态度\": \"未提及\", \"是否容易停车\": \"未提及\", \"点菜/上菜速度\": \"未提及\", \"价格水平\": \"未提及\", \"性价比\": \"未提及\", \"折扣力度\": \"未提及\", \"装修情况\": \"正面\", \"嘈杂情况\": \"中性\", \"就餐空间\": \"正面\", \"卫生情况\": \"正面\", \"分量\": \"未提及\", \"口感\": \"中性\", \"外观\": \"未提及\", \"推荐程度\": \"未提及\", \"本次消费感受\": \"正面\", \"再次消费的意愿\": \"正面\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'交通是否便利': '未提及',\n",
       " '距离商圈远近': '未提及',\n",
       " '是否容易寻找': '未提及',\n",
       " '排队等候时间': '未提及',\n",
       " '服务人员态度': '未提及',\n",
       " '是否容易停车': '未提及',\n",
       " '点菜/上菜速度': '未提及',\n",
       " '价格水平': '未提及',\n",
       " '性价比': '未提及',\n",
       " '折扣力度': '未提及',\n",
       " '装修情况': '正面',\n",
       " '嘈杂情况': '中性',\n",
       " '就餐空间': '正面',\n",
       " '卫生情况': '正面',\n",
       " '分量': '未提及',\n",
       " '口感': '中性',\n",
       " '外观': '未提及',\n",
       " '推荐程度': '未提及',\n",
       " '本次消费感受': '正面',\n",
       " '再次消费的意愿': '正面'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_path = '../XiaoWen/LLM-Tuning-master/weights/aspect_sentiment_jsonfy_output_10k_plus-internlm-chat-7b'\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "\n",
    "json.loads(predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! 重要经验\n",
    "generate参数的影响巨大，应该跟 modeling_xxx.py 中默认使用的参数对应，能保证比较好的效果！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"交通是否便利\": \"未提及\", \"距离商圈远近\": \"未提及\", \"是否容易寻找\": \"未提及\", \"排队等候时间\": \"未提及\", \"服务人员态度\": \"未提及\", \"是否容易停车\": \"未提及\", \"点菜/上菜速度\": \"正面\", \"价格水平\": \"未提及\", \"性价比\": \"未提及\", \"折扣力度\": \"未提及\", \"装修情况\": \"未提及\", \"嘈杂情况\": \"未提及\", \"就餐空间\": \"未提及\", \"卫生情况\": \"未提及\", \"分量\": \"未提及\", \"口感\": \"正面\", \"外观\": \"未提及\", \"推荐程度\": \"未提及\", \"本次消费感受\": \"正面\", \"再次消费的意愿\": \"未提及\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'点菜/上菜速度': '正面', '口感': '正面', '本次消费感受': '正面'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"很喜欢这家的餐，味道正宗，口感很好，是我喜欢的味道，满意值得推荐包装仔细，\\\n",
    "    味道很不错，小哥送的也很快，点赞！味道很惊喜，口味很赞👍相当的满足！\"\n",
    "output_d = json.loads(predict(text))\n",
    "\n",
    "output_d = {k:v for k,v in output_d.items() if v != '未提及'}\n",
    "output_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "答案：{\"pos\":0,\"neg\":1,\"neu\":0,\"text\": \"喝过最难喝的奶茶，一股水的味道，根本就喝不到奶茶味，全倒了，浪费。芋泥就一点点，珍珠就6小颗。服气了！差评，差评。\"}<eoa>\n",
      "\n",
      "答案：{\"pos\":0,\"neg\":1,\"neu\":0,\"text\": \"喝过最难喝的奶茶，一股水的味道，根本就喝不到奶茶味，全倒了，浪费。芋泥就一点点，珍珠就6小颗。服气了！差评，差评。\"}<eoa>\n"
     ]
    }
   ],
   "source": [
    "with model.disable_adapter(): # 禁用 lora\n",
    "    output = predict(text)\n",
    "    try:\n",
    "        output_d = json.loads(output)\n",
    "        output_d = {k:v for k,v in output_d.items() if v != '未提及'}\n",
    "        print(output_d)\n",
    "    except:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baichuan2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('../DCAI-share/llm/Baichuan2-7B-Base', trust_remote_code=True, device_map=\"auto\").half()\n",
    "tokenizer = AutoTokenizer.from_pretrained('../DCAI-share/llm/Baichuan2-7B-Base', trust_remote_code=True)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "# lora_path = 'weights/absa_instruction-first_d1_train-1000-Baichuan2-7B-Base'\n",
    "# model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "lora_path3 = 'weights/absa_lines-output_d1_train-1000-Baichuan2-7B-Base'\n",
    "model.load_adapter(lora_path3, adapter_name='lines-output_d1')\n",
    "model.set_adapter('lines-output_d1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食品评价：正面\n",
      "饮品评价：正面\n",
      "价格水平：正面\n",
      "卫生情况：正面\n",
      "服务人员态度：正面\n",
      "停车方便程度：未提及\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'食品评价：正面\\n饮品评价：正面\\n价格水平：正面\\n卫生情况：正面\\n服务人员态度：正面\\n停车方便程度：未提及'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "text = \"\"\"我们又来了，自从东方广场这间分店开张以来，我们五个人已经是第三次消费，有团购这么划算的优惠，就多来一些，反正到哪里都是吃的啦！人气比刚开始时回落一点，正常现象。服务都是一如既往的好，用团购券买单还可以享用抽奖中的沙律，满意。总结之前两次得出的经验，本人认为鸡皇牛扒比杂扒好吃太多，价钱好像差不多吧。鸡扒拌天使面，可能鸡扒太肥，感觉一般。照烧鱿鱼筒是每次都点的，值得推介。长豆角也很野味，不错。这次好抽中两份奖品，要准备好下一次的钱了，再优惠些吧！\\n---\\n阅读上面这段评论，观察以下这些方面：食品评价，饮品评价，价格水平，卫生情况，服务人员态度，停车方便程度。请根据评论对这些方面进行情感分析，具体有四类情感：正面、负面、中性、未提及。请用以下格式给出所有方面的情感：\\\"方面1：情感类别\\n方面2：情感类别\\n...\\\"\\n输出：\"\"\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = inputs.to('cuda:0')\n",
    "output = model.generate(**inputs, max_new_tokens=128,repetition_penalty=1.1, streamer=streamer)\n",
    "output = output[0][inputs.input_ids.shape[1]:] # 这样可以防止输出prompt部分\n",
    "tokenizer.decode(output,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese-LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "from transformers import AutoModel, AutoModelForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('../DCAI-share/llm/chinese-llama-2-7b', trust_remote_code=True, device_map=\"auto\") \n",
    "tokenizer = LlamaTokenizer.from_pretrained('../DCAI-share/llm/chinese-llama-2-7b', trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，我是来自湖南的一名高三学生。我今年17岁，身高168cm，体重50kg左右，属于偏瘦型身材。请问我的体型适合穿什么类型的衣服呢？谢谢！你好，你的体型是标准的骨架型，比较适合穿修身的衣服，比如：衬衫、西服等。\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "text = '你好啊'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = inputs.to('cuda:0')\n",
    "output = model.generate(**inputs, max_new_tokens=128,repetition_penalty=1.1, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraModel\n",
    "# lora_path = 'weights/absa_baseline_d1_train-500-llama2'\n",
    "# model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "text = '你好啊'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = inputs.to('cuda:0')\n",
    "output = model.generate(**inputs, max_new_tokens=128,repetition_penalty=1.1, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.8/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /root/anaconda3/envs/gby did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(55296, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=55296, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('../DCAI-share/llm/chinese-alpaca-2-7b', trust_remote_code=True, device_map=\"auto\") \n",
    "tokenizer = AutoTokenizer.from_pretrained('../DCAI-share/llm/chinese-alpaca-2-7b', trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xyc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19ae5496032f21c9f9bf62c0f263baec2e36b670b1564db611f67406e49e376d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
